<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-22. Spark字符统计示例" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/22.%20Spark%E5%AD%97%E7%AC%A6%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B/" class="article-date">
  <time datetime="2020-06-19T05:44:02.951Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/22.%20Spark%E5%AD%97%E7%AC%A6%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B/">22. Spark字符统计示例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="22-Spark字符统计示例"><a href="#22-Spark字符统计示例" class="headerlink" title="22. Spark字符统计示例"></a>22. Spark字符统计示例</h1><p>在Spark字符统计示例中，将找出指定文件中每个字符的频率。这里使用scala语言来执行spark操作。</p>
<h4 id="执行Spark字符计数示例的步骤"><a href="#执行Spark字符计数示例的步骤" class="headerlink" title="执行Spark字符计数示例的步骤"></a>执行Spark字符计数示例的步骤</h4><p>写好一个sparkdata.txt:</p>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122541.png" alt="image-20200612120827865"></p>
<p>在hadoop的目录中创建一个目录，保存文本文件：</p>
<blockquote>
<p>$ hdfs dfs -mkdir /spark</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612114904.png" alt="image-20200612114904720"></p>
<p>将sparkdata.txt文件上传到特定目录中：</p>
<blockquote>
<p>$ hdfs dfs -put /root/sparkdata.txt /spark</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115106.png" alt="image-20200612115106288"></p>
<p>在scala模式下打开spark:</p>
<blockquote>
<p>$ spark-shell</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122530.png" alt="image-20200612120933126"></p>
<p>创建一个RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.textFile(“../../sparkdata.txt”)</p>
</blockquote>
<p>这里传递包含数据的文件名。</p>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115806.png" alt="image-20200612115806248"></p>
<p>以单个字符的形式拆分现有数据：</p>
<blockquote>
<p>scala&gt; val splitdata  = data.flatMap(line =&gt; line.split(“”))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; splitdata.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612121913.png" alt="image-20200612121913070"></p>
<p>执行映射操作：</p>
<blockquote>
<p>scala&gt; val mapdata = splitdata.map(word =&gt; (word, 1))</p>
</blockquote>
<p>这里给每个字符分配了一个值<strong>1</strong>。</p>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; mapdata.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122053.png" alt="image-20200612122052952"></p>
<p>执行<code>reduce</code>操作：</p>
<blockquote>
<p>scala&gt; val reducedata = mapdata.reduceByKey(_+_)</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; reducedata.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122456.png" alt="image-20200612122456444"></p>
<p>返回结果是计算所有字符及它们的出现次数。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/22.%20Spark%E5%AD%97%E7%AC%A6%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B/" data-id="ckblsgie4000gxwtq0fod2cfn" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-21. Spark单词统计示例（word count）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/21.%20Spark%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B%EF%BC%88word%20count%EF%BC%89/" class="article-date">
  <time datetime="2020-06-19T05:44:02.939Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/21.%20Spark%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B%EF%BC%88word%20count%EF%BC%89/">21. Spark单词统计示例（word count）</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="21-Spark单词统计示例（word-count）"><a href="#21-Spark单词统计示例（word-count）" class="headerlink" title="21. Spark单词统计示例（word count）"></a>21. Spark单词统计示例（word count）</h1><p>在Spark单词统计示例中，将找出指定文件中存在的每个单词的出现频率。这里我们使用Scala语言来执行Spark操作。</p>
<h4 id="执行Spark字数计算示例的步骤"><a href="#执行Spark字数计算示例的步骤" class="headerlink" title="执行Spark字数计算示例的步骤"></a>执行Spark字数计算示例的步骤</h4><p>在此示例中，查找并显示每个单词的出现次数。在本地服务器中创建一个文本并在其中写入一些单词。</p>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612114032.png" alt="image-20200612114032274"></p>
<p>在hadoop的目录中创建一个目录，保存文本文件：</p>
<blockquote>
<p>$ hdfs dfs -mkdir /spark</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612114904.png" alt="image-20200612114904720"></p>
<p>将sparkdata.txt文件上传到特定目录中：</p>
<blockquote>
<p>$ hdfs dfs -put /root/sparkdata.txt /spark</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115106.png" alt="image-20200612115106288"></p>
<p>在Scala模式下打开Spark:</p>
<blockquote>
<p>$ spark-shell</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115211.png" alt="image-20200612115211199"></p>
<p>创建一个RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.textFile(“../../sparkdata.txt”)</p>
</blockquote>
<p>这里传递包含数据的任何文件名。</p>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115806.png" alt="image-20200612115806248"></p>
<p>以单个单词的形式拆分现有数据（制表符作为分隔符）：</p>
<blockquote>
<p>scala&gt; val splitdata = data.flatMap(line =&gt; line.split(“\t”))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; splitdata.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612120037.png" alt="image-20200612120037176"></p>
<p>执行映射操作：</p>
<blockquote>
<p>scala&gt; val mapdata =  splitdata.map(word =&gt; (word, 1))</p>
</blockquote>
<p>这里每个单词分配值<strong>1</strong>。</p>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; mapdata.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122613.png" alt="image-20200612120241496"></p>
<p>每个单词都有了一个值1.</p>
<p>现在执行<code>reduce</code>操作：</p>
<blockquote>
<p>scala&gt; val reducedata = mapdata.reduceByKey(_+_)</p>
</blockquote>
<p>这里汇总了生成的数据。</p>
<p>读取生成的数据：</p>
<blockquote>
<p>scala&gt; reducedata.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122622.png" alt="image-20200612120428781"></p>
<p>返回结果就是每个单词和他们出现的次数。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/21.%20Spark%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B%EF%BC%88word%20count%EF%BC%89/" data-id="ckblsgie4000fxwtqfwov4yh0" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-20. Spark Take函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/20.%20Spark%20Take%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:44:02.886Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/20.%20Spark%20Take%E5%87%BD%E6%95%B0/">20. Spark Take函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="20-Spark-Take函数"><a href="#20-Spark-Take函数" class="headerlink" title="20. Spark Take函数"></a>20. Spark Take函数</h1><p>在Spark中，<code>take</code>函数的行为类似于数组。它接收一个整数值（比如n）作为参数，并返回数据集的前<code>n</code>个元素的数组。</p>
<h4 id="Take函数示例"><a href="#Take函数示例" class="headerlink" title="Take函数示例"></a>Take函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.parallelize(List(10, 20, 30, 40, 50))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect()</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612113155.png" alt="image-20200612113155754"></p>
<p>应用<code>take()</code>函数来检索数据集的第一个元素（类似first()函数）和前三个元素：</p>
<blockquote>
<p>scala&gt; val takefunc = data.take(1)</p>
<p>scala&gt; val takefunc = data.take(3)</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612113432.png" alt="image-20200612113432394"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/20.%20Spark%20Take%E5%87%BD%E6%95%B0/" data-id="ckblsgie00009xwtq165y4erd" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-19. Spark First函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/19.%20Spark%20First%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:44:02.884Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/19.%20Spark%20First%E5%87%BD%E6%95%B0/">19. Spark First函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="19-Spark-First函数"><a href="#19-Spark-First函数" class="headerlink" title="19. Spark First函数"></a>19. Spark First函数</h1><p>在Spark中，<code>First</code>函数始终返回数据集的第一个元素。它类似于<code>take(1)</code>。</p>
<h4 id="First函数示例"><a href="#First函数示例" class="headerlink" title="First函数示例"></a>First函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.parallelize(List(10, 20, 30, 40, 50))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612112756.png" alt="image-20200612112756704"></p>
<p>使用<code>first()</code>函数来检索数据集的第一个元素：</p>
<blockquote>
<p>scala&gt; val firstfunc = data.first()</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612112858.png" alt="image-20200612112857994"></p>
<p>返回数据集的第一个元素：10</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/19.%20Spark%20First%E5%87%BD%E6%95%B0/" data-id="ckblsgidy0007xwtqhl7qbrz6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-18. Spark cogroup函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/18.%20Spark%20cogroup%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:44:02.879Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/18.%20Spark%20cogroup%E5%87%BD%E6%95%B0/">18. Spark cogroup函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="18-Spark-cogroup函数"><a href="#18-Spark-cogroup函数" class="headerlink" title="18. Spark cogroup函数"></a>18. Spark cogroup函数</h1><p>在Spark中，<code>cogroup</code>函数对不同的数据集执行分组操作，比如，对(K, V)和(K, W)执行cogroup并返回<code>(K, (Iterable, Iterable))</code>元祖的数据集。此操作也称为<code>groupWith</code>。</p>
<h4 id="cogroup函数示例"><a href="#cogroup函数示例" class="headerlink" title="cogroup函数示例"></a>cogroup函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data1 =  sc.parallelize(Seq((“A”, 1), (“B”, 2), (“C”, 3)))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data1.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612111743.png" alt="image-20200612111743297"></p>
<p>使用并行化集合创建另一个RDD：</p>
<blockquote>
<p>scala&gt; val data2 = sc.parallelize(Seq((“B”, 4), (“E”, 5)))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data2.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612111949.png" alt="image-20200612111949225"></p>
<p>使用<code>cogroup()</code>函数对值进行分组：</p>
<blockquote>
<p>scala&gt; val cogroupfunc = data1.cogroup(data2)</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; cogroupfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612112131.png" alt="image-20200612112131504"></p>
<p>返回的结果是把两个数据集中的所有键包含的值分组在一起。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/18.%20Spark%20cogroup%E5%87%BD%E6%95%B0/" data-id="ckblsgidz0008xwtqdg907oyz" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-17. Spark reduceByKey函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/17.%20Spark%20reduceByKey%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:44:02.831Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/17.%20Spark%20reduceByKey%E5%87%BD%E6%95%B0/">17. Spark reduceByKey函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="17-Spark-reduceByKey函数"><a href="#17-Spark-reduceByKey函数" class="headerlink" title="17. Spark reduceByKey函数"></a>17. Spark reduceByKey函数</h1><p>在spark中，<code>reduceByKey</code>函数是一种常用的转换操作，它执行数据聚合（聚合的规则自己定义）。</p>
<p>它接收键值对(K, V)作为输入，基于键聚合值并生成(K, V）对的数据集作为输出。</p>
<h4 id="reduceByKey函数示例"><a href="#reduceByKey函数示例" class="headerlink" title="reduceByKey函数示例"></a>reduceByKey函数示例</h4><p>在此示例中，我们基于键聚合值.</p>
<p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.parallelize(Array((“C”, 3), (“A”, 1), (“B”, 4), (“A”, 2), (“B”, 5)))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612111753.png" alt="image-20200612105256248"></p>
<p>应用<code>reduceByKey()</code>函数来聚合值。</p>
<blockquote>
<p>scala&gt; val reducefunc = data.reduceByKey((value, x) =&gt; (value + x))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; reducefunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612111805.png" alt="image-20200612105529453"></p>
<p>这里聚合的规则是把键相同的值都加起来，返回结果是键值对，每个键的值是原来数据集中这个键的所有值的总和。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/17.%20Spark%20reduceByKey%E5%87%BD%E6%95%B0/" data-id="ckblsgie1000axwtq9wethu1d" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-16. Spark groupByKey函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/16.%20Spark%20groupByKey%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:44:02.760Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/16.%20Spark%20groupByKey%E5%87%BD%E6%95%B0/">16. Spark groupByKey函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="16-Spark-groupByKey函数"><a href="#16-Spark-groupByKey函数" class="headerlink" title="16. Spark groupByKey函数"></a>16. Spark groupByKey函数</h1><p>在spark中，<code>groupByKey</code>函数是一种经常使用的转换操作，它执行数据的<strong>混乱</strong>。</p>
<p>它接收键值对(K, V)作为输入，基于键值对进行分组，<strong><em>key相同的value被分到一组</em></strong>。并生成（K, Iterable）对的数据集作为输出。</p>
<h4 id="groupByKey函数示例"><a href="#groupByKey函数示例" class="headerlink" title="groupByKey函数示例"></a>groupByKey函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.parallelize(Seq((“C”, 3), (“A”, 1), (“ B”, 4), (“A”, 2), (“B”, 5)))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612104346.png" alt="image-20200612104346925"></p>
<p>应用<code>groupByKey()</code>函数对键值对进行分组：</p>
<blockquote>
<p>scala&gt; val groupfunc = data.groupByKey()</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; groupfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612104534.png" alt="image-20200612104534462"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/16.%20Spark%20groupByKey%E5%87%BD%E6%95%B0/" data-id="ckblsgie2000cxwtqdaipa1jn" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-15. Spark sortByKey函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/15.%20Spark%20sortByKey%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:44:02.690Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/15.%20Spark%20sortByKey%E5%87%BD%E6%95%B0/">15. Spark sortByKey函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="15-Spark-sortByKey函数"><a href="#15-Spark-sortByKey函数" class="headerlink" title="15. Spark sortByKey函数"></a>15. Spark sortByKey函数</h1><p>在Spark中，<code>sortByKey</code>函数维护元素的顺序。它接收键值对<code>(K, V)</code>作为输入，按升序或降序对元素进行排序，并按顺序生成数据集。</p>
<h4 id="sortByKey函数示例"><a href="#sortByKey函数示例" class="headerlink" title="sortByKey函数示例"></a>sortByKey函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.parallelize(Seq((“C”, 3), (“A”, 1), (“D”, 4), (“B”, 2), (“E”, 5)))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612103346.png" alt="image-20200612103346422"></p>
<p>应用<code>sortByKey()</code>函数来对数据集进行排序：</p>
<blockquote>
<p>scala&gt; val sortfunc = data.sortByKey()</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; sortfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612103512.png" alt="image-20200612103512106"></p>
<p>返回结果按照K-V对的key进行排序，这里的key是字母，返回结果按照字典序升序排序。</p>
<p>对于降序，应用<code>sortByKey()</code>函数并将布尔类型作为参数传递。</p>
<blockquote>
<p>scala&gt; val sortfunc = data.sortByKey(false)</p>
</blockquote>
<p>读取生成的结果:</p>
<blockquote>
<p>scala&gt; sortfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612103722.png" alt="image-20200612103722121"></p>
<p>sortByKey函数传递一个<code>false</code>参数，最后的返回结果就是按照字典序降序。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/15.%20Spark%20sortByKey%E5%87%BD%E6%95%B0/" data-id="ckblsgidy0006xwtq6jwghsj6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-14. Spark Cartesian函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/14.%20Spark%20Cartesian%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:44:02.658Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/14.%20Spark%20Cartesian%E5%87%BD%E6%95%B0/">14. Spark Cartesian函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="14-Spark-Cartesian函数"><a href="#14-Spark-Cartesian函数" class="headerlink" title="14. Spark Cartesian函数"></a>14. Spark Cartesian函数</h1><p>在Spark中，<code>cartesian</code>函数生成两个数据集的笛卡尔积，并返回所有可能的组合对。</p>
<p>这里，一个数据集的每一个元素都与另一个数据集的每个元素配对。</p>
<h4 id="Cartesian函数示例"><a href="#Cartesian函数示例" class="headerlink" title="Cartesian函数示例"></a>Cartesian函数示例</h4><p>在此示例中，生成两个数据的笛卡尔积。</p>
<p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data1 = sc.parallelize(List(1, ,2, 3))</p>
</blockquote>
<p>读取生成的结果:</p>
<blockquote>
<p>scala&gt; data1.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612101947.png" alt="image-20200612101947084"></p>
<p>使用并行化集合创建另一个RDD：</p>
<blockquote>
<p>scala&gt; val data2 = sc.parallelize(List(3, 4, 5))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data2.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612102109.png" alt="image-20200612102109091"></p>
<p>应用<code>cartesian()</code>函数返回两个数据集的笛卡尔积:</p>
<blockquote>
<p>scala&gt; val cartesianfunc = data1.cartesian(data2)</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; cartesianfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612102315.png" alt="image-20200612102315863"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/14.%20Spark%20Cartesian%E5%87%BD%E6%95%B0/" data-id="ckblsgidx0005xwtqho8w1g6l" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-13. Spark Intersection函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/13.%20Spark%20Intersection%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:44:02.598Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/13.%20Spark%20Intersection%E5%87%BD%E6%95%B0/">13. Spark Intersection函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="13-Spark-Intersection函数"><a href="#13-Spark-Intersection函数" class="headerlink" title="13. Spark Intersection函数"></a>13. Spark Intersection函数</h1><p>在Spark中，<code>Intersection</code>函数返回一个新数据集，其中包含不同数据集中存在的元素的<strong><em>交集</em></strong>。因此它只返回一行。此函数的行为与SQL中的<code>INTERSECT</code>查询类似。</p>
<h4 id="Intersection函数示例"><a href="#Intersection函数示例" class="headerlink" title="Intersection函数示例"></a>Intersection函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data1 = sc.parallelzie(List(1, 2 , 3))</p>
</blockquote>
<p>读取生成的结果:</p>
<blockquote>
<p>scala&gt; data1.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612100650.png" alt="image-20200612100650624"></p>
<p>使用并行化集合创建另一个RDD：</p>
<blockquote>
<p>scala&gt; val data2 = sc.parallelize(List(3, 4, 5))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data2.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612100804.png" alt="image-20200612100804900"></p>
<p>应用<code>intersection()</code>函数返回两个数据集的交集:</p>
<blockquote>
<p>scala&gt; val intersectfunc = data1.intersection(data2)</p>
</blockquote>
<p>读取生成的结果:</p>
<blockquote>
<p>scala&gt; intersectfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612100934.png" alt="image-20200612100933990"></p>
<p>返回结果是两个集合的交集: 3</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/13.%20Spark%20Intersection%E5%87%BD%E6%95%B0/" data-id="ckblsgidu0001xwtqftzgdw56" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/06/19/22.%20Spark%E5%AD%97%E7%AC%A6%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B/">22. Spark字符统计示例</a>
          </li>
        
          <li>
            <a href="/2020/06/19/21.%20Spark%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B%EF%BC%88word%20count%EF%BC%89/">21. Spark单词统计示例（word count）</a>
          </li>
        
          <li>
            <a href="/2020/06/19/20.%20Spark%20Take%E5%87%BD%E6%95%B0/">20. Spark Take函数</a>
          </li>
        
          <li>
            <a href="/2020/06/19/19.%20Spark%20First%E5%87%BD%E6%95%B0/">19. Spark First函数</a>
          </li>
        
          <li>
            <a href="/2020/06/19/18.%20Spark%20cogroup%E5%87%BD%E6%95%B0/">18. Spark cogroup函数</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>