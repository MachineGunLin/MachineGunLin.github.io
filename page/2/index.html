<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Spark学习笔记/6. RDD持久化" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%20RDD%E6%8C%81%E4%B9%85%E5%8C%96/" class="article-date">
  <time datetime="2020-06-19T05:47:23.174Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%20RDD%E6%8C%81%E4%B9%85%E5%8C%96/">Spark学习笔记/6. RDD持久化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="6-RDD持久化"><a href="#6-RDD持久化" class="headerlink" title="6. RDD持久化"></a>6. RDD持久化</h1><p>Spark通过在操作中将数据持久保存在内存中，提供了一种处理数据集的便捷方式。</p>
<p>在持久化RDD的同时，每个节点都存储它在内存中计算的任何分区，也可以在该数据集的其他任务中重用他们。</p>
<p>可以使用<code>persist()</code>或<code>cache(0)</code>方法来标记要保留的RDD。Spark的缓存是容错的。在任何情况下，如果RDD的分区丢失，spark将使用最初创建它的<code>转换</code>自动重新计算。</p>
<p>存在可用于存储持久RDD的不同存储级别。通过将<code>StorageLevel</code>对象（Scala，java，python）传递给<code>persist()</code>来使用这些级别。<code>cache()</code>方法用于默认存储级别，即<code>StorageLevel.MEMORY_ONLY</code>。</p>
<p>存储级别的集合：</p>
<table>
<thead>
<tr>
<th align="center">存储级别</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>MEMORY_ONLY</code></td>
<td align="center">将RDD存储为JVM中的反序列化Java对象。这是默认级别。如果RDD不适合内存，则每次需要时都不会缓存和重新计算某些分区。</td>
</tr>
<tr>
<td align="center"><code>MEMORY_ADD_DISK</code></td>
<td align="center">它将RDD存储为JVM中反序列化Java对象。如果RDD不适合内存，请存储在适合磁盘的分区，并在需要时从磁盘读取它们。</td>
</tr>
<tr>
<td align="center"><code>MEMORY_ONLY_SER</code></td>
<td align="center">它将RDD存储为序列化Java对象（即每个分区一个字节的数组）。这通常比反序列对象更节省空间。</td>
</tr>
<tr>
<td align="center"><code>MEMORY_ADD_DISK_SER</code></td>
<td align="center">类似于<code>MEMORY_DISK_SER</code>，但是将内存中不适合的分区溢出到磁盘而不是重新计算它们。</td>
</tr>
<tr>
<td align="center"><code>DISK_ONLY</code></td>
<td align="center">它仅将RDD分区存储在磁盘上。</td>
</tr>
<tr>
<td align="center"><code>MEMORY_ONLY_2</code>, <code>MEMORY_AND_DISK_2</code></td>
<td align="center">它与上面的级别相同，但复制两个集群上的每个分区。</td>
</tr>
<tr>
<td align="center"><code>OFF_HEAP</code></td>
<td align="center">类似于<code>MEMORY_ONLY_SER</code>，但将数据存储在堆外内存中。必须启用堆外内存。</td>
</tr>
</tbody></table>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%20RDD%E6%8C%81%E4%B9%85%E5%8C%96/" data-id="ckblska72000jlktqfbey2e5f" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/5.RDD操作" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.RDD%E6%93%8D%E4%BD%9C/" class="article-date">
  <time datetime="2020-06-19T05:47:23.129Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.RDD%E6%93%8D%E4%BD%9C/">Spark学习笔记/5.RDD操作</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="5-RDD操作"><a href="#5-RDD操作" class="headerlink" title="5.RDD操作"></a>5.RDD操作</h1><p>RDD提供两种类型的操作： </p>
<ol>
<li>转换</li>
<li>行动</li>
</ol>
<h4 id="1-转换"><a href="#1-转换" class="headerlink" title="1. 转换"></a>1. 转换</h4><p>在Spark中，转换的作用是从现有数据集创建新数据集。</p>
<p>转换是惰性的，它们仅在动作需要将结果返回到驱动程序时才计算。</p>
<p>常用的RDD转换：</p>
<ul>
<li><code>map(func)</code> ： 它返回一个新的分布式数据集，该数据集是通过<code>func</code>传递源的每个元素而形成的。</li>
<li><code>filter(func)</code> ： 它返回一个新数据集，该数据集是通过选择函数<code>func</code>返回<code>true</code>的源元素而形成的。</li>
<li><code>flatMap(func)</code> ： 这里每个输入项可以映射到零个或多个输出项，因此函数<code>func</code>应该返回序列而不是单个项。</li>
<li><code>mapPartitions(func)</code> ： 类似于<code>map(func)</code>，但是它是在RDD的每个分区（块）上单独运行，因此当在类型T的RDD上运行时，<code>func</code>必须是<code>Iterator &lt;T&gt; =&gt; Iterator &lt;U&gt;</code>类型。</li>
<li><code>mapPartitionsWithIndex(func)</code> ： 类似于<code>mapPartitons(func)</code>，它为<code>func</code>提供了一个表示分区索引的整数值，因此当在类型T的RDD上运行时，<code>func</code>必须是类型<code>(Int, Iterator &lt;T&gt;)=&gt; Iterator &lt;U&gt;</code>。</li>
<li><code>sample(withReplacement, fraction, seed)</code> ： 它使用给定的随机数生成器种子对数据的分数部分进行采样，<code>withReplacement</code>表示有或者没有替换。</li>
<li><code>union(otherDataset)</code> ： 它返回一个新数据集，其中包含源数据集和参数中元素的并集。</li>
<li><code>intersection(otherDataset)</code> ： 它返回一个新RDD，其中包含源数据集和参数中的集合元素的交集。</li>
<li><code>distinct([numPartitions]))</code> ： 它返回一个新数据集，其中包含源数据集的不同元素。</li>
<li><code>groupByKey([numPartitions])</code> ：当在<code>(K, V)</code>对的数据集上调用时，它返回<code>(K, Iterable)</code>对的数据集。</li>
<li><code>reduceByKey(func, [numPartitions])</code> ： 当调用<code>(K, V)</code>对的数据集时，返回<code>(K, V)</code>对的数据集，其中使用给定的<code>reduce</code>函数<code>func</code>聚合每个键的值，该函数类型必须是<code>(V, V)=&gt;V</code>。</li>
<li><code>aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])</code>  ： 当调用<code>(K, V)</code>对的数据集时，返回<code>(K, V)</code>对的数据集，其中使用给定的组合函数和中性”零“值聚合每个键的值。</li>
<li><code>sortByKey([ascending], [numPartitions])</code> ： 它返回按键值升序或降序排序的键值对的数据集，如在布尔<code>ascending</code>参数中指定。</li>
<li><code>join(otherDataset, [numPartitions])</code> ： 当调用类型<code>(K, V)</code>和<code>(K, W)</code>的数据集时，返回<code>(K, (V, W))</code>对的数据集以及每个键的所有元素对。通过<code>leftOuterJoin</code>，<code>rightOuterJoin</code>和<code>fullOuterJoin</code>支持外连接。</li>
<li><code>cogroup(otherDataset, [numPartitions])</code> ： 当调用类型<code>(K, V)</code>和<code>(K, W)</code>的数据集时，返回<code>(K, (Iterable, Iterable))</code>元祖的数据集。此操作也成为<code>groupWith</code>。</li>
<li><code>cartersian(otherDataset)</code> ： 当调用类型为T和U的数据集时，返回<code>(T, U)</code>对的数据集（所有元素对）。</li>
<li><code>pipe(command, [envVars])</code> ： 通过shell命令管道RDD的每个分区，例如一个perl或bash脚本。</li>
<li><code>coalesce(numPartitions)</code> ： 它将RDD中的分区数减少到<code>numPartitions</code>。</li>
<li><code>repartition(numPartitions)</code> ： 它随机重新调整RDD中的数据，以创建更多或更少的分区，并在它们之间进行平衡。</li>
<li><code>repartitionAndSortWithinPartitions(partitioner)</code> ： 它根据给定的分区器对RDD进行重新分区，并在每个生成的分区中的键值对记录进行排序。</li>
</ul>
<h4 id="2-操作"><a href="#2-操作" class="headerlink" title="2. 操作"></a>2. 操作</h4><p>在Spark中，操作的作用是在对数据集运行计算后将值返回给驱动程序。</p>
<p>常用的RDD操作：</p>
<table>
<thead>
<tr>
<th align="center">操作</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>reduce(func)</code></td>
<td align="center">它使用函数func(它接受两个参数并返回一个)来聚合数据集的元素。该函数应该是可交换的和可关联的，以便可以并行正确计算。</td>
</tr>
<tr>
<td align="center"><code>collect()</code></td>
<td align="center">它将数据集的所有元素作为数组返回到驱动程序中。在过滤器或其他返回足够小的数据子集的操作之后，这通常很有用。</td>
</tr>
<tr>
<td align="center"><code>count(func)</code></td>
<td align="center">返回数据集中的元素个数。</td>
</tr>
<tr>
<td align="center"><code>first()</code></td>
<td align="center">返回数据集的第一个元素（类似于<code>take(1)</code>）。</td>
</tr>
<tr>
<td align="center"><code>take(n)</code></td>
<td align="center">返回一个包含数据集的前n个元素的数组。</td>
</tr>
<tr>
<td align="center"><code>takeSample(withReplcament, num, [seed])</code></td>
<td align="center">返回一个数组，其中包含数据集的num个元素的随机样本，withReplacement表示有或者没有替换，seed是可选的，可以预先指定随机数生成器种子。</td>
</tr>
<tr>
<td align="center"><code>takeOrdered(n, [ordering])</code></td>
<td align="center">它使用自然顺序或自定义比较器[ordering]返回RDD的前n个元素。</td>
</tr>
<tr>
<td align="center"><code>saveAsTextFile(path)</code></td>
<td align="center">用于将数据集的元素作为文本文件（或文本文件集）写入本地文件系统，HDFS或任何其他Hadoop支持的文件系统的给定目录中。</td>
</tr>
<tr>
<td align="center"><code>saveAsSequenceFile(path)</code></td>
<td align="center">它用于在本地文件系统，HDFS或任何其他Hadoop支持的文件系统中的给定路径中将数据集的元素编写为Hadoop SequenceFile。</td>
</tr>
<tr>
<td align="center"><code>saveAsObjectFile(path)</code></td>
<td align="center">它用于使用java序列化以简单格式编写数据集的元素，然后可以使用<code>SparkContext.objectFile()</code>加载。</td>
</tr>
<tr>
<td align="center"><code>countByKey()</code></td>
<td align="center">它仅适用于类型(K, V)的RDD。因此它返回（K, Int）对的散列映射与每个键的计数。</td>
</tr>
<tr>
<td align="center"><code>foreach(func)</code></td>
<td align="center">它在数据集的每个元素上运行函数<code>func</code>以获得副作用，例如更新累加器或与外部存储系统交互。</td>
</tr>
</tbody></table>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.RDD%E6%93%8D%E4%BD%9C/" data-id="ckblska70000hlktq44ykeshj" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/4.弹性分布式数据集（RDD）简介" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4.%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88RDD%EF%BC%89%E7%AE%80%E4%BB%8B/" class="article-date">
  <time datetime="2020-06-19T05:47:23.058Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4.%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88RDD%EF%BC%89%E7%AE%80%E4%BB%8B/">Spark学习笔记/4.弹性分布式数据集（RDD）简介</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="4-弹性分布式数据集（RDD）简介"><a href="#4-弹性分布式数据集（RDD）简介" class="headerlink" title="4.弹性分布式数据集（RDD）简介"></a>4.弹性分布式数据集（RDD）简介</h1><p><strong><em>弹性分布式数据集（RDD）是Spark的核心抽象之一。它是一组元素，在集群的节点之间进行分区，以便我们可以对其执行各种并行操作。</em></strong></p>
<p>创建RDD有两种方法：</p>
<ol>
<li>并行化驱动程序中的现有数据。</li>
<li>引用外部存储系统中的数据集。例如：共享文件系统、HDFS、HBASE或者提供Hadoop InputFormat的数据源。</li>
</ol>
<h4 id="1-并行化集合"><a href="#1-并行化集合" class="headerlink" title="1.并行化集合"></a>1.并行化集合</h4><p>要创建并行化集合，在驱动程序的现有集合上调用<code>SparkContext</code>的<code>parallelize</code>方法。</p>
<p>复制集合中的每个元素以形成可并行操作的分布式数据集。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var info &#x3D; Array(1, 2, 3, 4)</span><br><span class="line">var distinfo &#x3D; sc.parallelize(info)</span><br></pre></td></tr></table></figure>

<p>现在，可以操作分布式数据集distinfo，例如<code>distinfo.reduce((a, b) =&gt; a + b)</code></p>
<h4 id="2-外部数据集"><a href="#2-外部数据集" class="headerlink" title="2.外部数据集"></a>2.外部数据集</h4><p>在Spark中，可以从Hadoop支持的任何类型的存储源（如HDFS，Cassandra, HBase甚至本地文件系统）创建分布式数据集。Spark提供对文本文件，<code>SqeuenceFiles</code>和其他类型的Hadoop InputFormat的支持。</p>
<p><code>SparkContext</code>的<code>textFile</code>方法可用于创建RDD的文本文件。此方法获取文件的URL（计算机上的本地路径或<code>hdfs://</code>)并读取文件的数据。</p>
<p>可以通过数据集来操作数据。例如使用<code>map</code>和<code>reduceoperations</code>来添加所有行的大小：<code>data.map(s =&gt; s.length).reduce((a, b) =&gt; a + b)</code></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4.%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88RDD%EF%BC%89%E7%AE%80%E4%BB%8B/" data-id="ckblska70000glktqgwh5cd36" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/3.Spark组件" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.Spark%E7%BB%84%E4%BB%B6/" class="article-date">
  <time datetime="2020-06-19T05:47:22.953Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.Spark%E7%BB%84%E4%BB%B6/">Spark学习笔记/3.Spark组件</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="3-Spark组件"><a href="#3-Spark组件" class="headerlink" title="3.Spark组件"></a>3.Spark组件</h1><p>Spark是一个计算引擎，可以组织、分发和监控多个应用程序。Spark由不同类型的组件组成：</p>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200611160912.png" alt="image-20200611155359323"></p>
<h4 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h4><p>Spark Core是Spark的核心，执行spark的核心功能。</p>
<p>Spark Core包含用于任务调度，故障恢复，存储系统和内存管理交互的组件。</p>
<h4 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h4><ul>
<li>Spark SQL构建于Spark Core之上，它为结构化数据（structured data）提供支持。</li>
<li>Spark SQL允许通过SQL以及SQL的Apache Hive变体HQL（Hive查询语言）查询数据。</li>
<li>它支持JDBC和ODBC连接，这些连接建立java对象与现有数据库，数据仓库和商业智能工具之间的关系。</li>
<li>它还支持各种数据源，如Hive表，Parquet和JSON。</li>
</ul>
<h4 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h4><ul>
<li>支持流数据的可伸缩和容错处理。</li>
<li>它使用Spark Core的快速调度功能来执行流分析。</li>
<li>它接受小批量数据并对数据执行RDD转换。</li>
<li>Spark Streaming的设计确保为流数据编写的应用程序可以重复使用，只需要很少的修改即可分析批量的历史数据。</li>
<li>Web服务器生成的日志文件可以视为数据流的实时示例。</li>
</ul>
<h4 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h4><p>MLlib是一个包含各种机器学习算法的机器学习库。它包含相关性和假设检验、分类和回归、聚类和主成分分析等。它比Apache Mahout使用的基于磁盘的实现快9倍。</p>
<h4 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h4><p>GraphX是一个用于操作图形和执行图形并行计算的库。它可以创建一个有向图，每个顶点和边可以附加任意属性。</p>
<p>要操纵图形，它支持各种基本运算符，如子图、连接顶点和聚合消息。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.Spark%E7%BB%84%E4%BB%B6/" data-id="ckblska6z000flktq03beded0" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/22. Spark字符统计示例" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/22.%20Spark%E5%AD%97%E7%AC%A6%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B/" class="article-date">
  <time datetime="2020-06-19T05:47:22.653Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/22.%20Spark%E5%AD%97%E7%AC%A6%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B/">Spark学习笔记/22. Spark字符统计示例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="22-Spark字符统计示例"><a href="#22-Spark字符统计示例" class="headerlink" title="22. Spark字符统计示例"></a>22. Spark字符统计示例</h1><p>在Spark字符统计示例中，将找出指定文件中每个字符的频率。这里使用scala语言来执行spark操作。</p>
<h4 id="执行Spark字符计数示例的步骤"><a href="#执行Spark字符计数示例的步骤" class="headerlink" title="执行Spark字符计数示例的步骤"></a>执行Spark字符计数示例的步骤</h4><p>写好一个sparkdata.txt:</p>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122541.png" alt="image-20200612120827865"></p>
<p>在hadoop的目录中创建一个目录，保存文本文件：</p>
<blockquote>
<p>$ hdfs dfs -mkdir /spark</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612114904.png" alt="image-20200612114904720"></p>
<p>将sparkdata.txt文件上传到特定目录中：</p>
<blockquote>
<p>$ hdfs dfs -put /root/sparkdata.txt /spark</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115106.png" alt="image-20200612115106288"></p>
<p>在scala模式下打开spark:</p>
<blockquote>
<p>$ spark-shell</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122530.png" alt="image-20200612120933126"></p>
<p>创建一个RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.textFile(“../../sparkdata.txt”)</p>
</blockquote>
<p>这里传递包含数据的文件名。</p>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115806.png" alt="image-20200612115806248"></p>
<p>以单个字符的形式拆分现有数据：</p>
<blockquote>
<p>scala&gt; val splitdata  = data.flatMap(line =&gt; line.split(“”))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; splitdata.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612121913.png" alt="image-20200612121913070"></p>
<p>执行映射操作：</p>
<blockquote>
<p>scala&gt; val mapdata = splitdata.map(word =&gt; (word, 1))</p>
</blockquote>
<p>这里给每个字符分配了一个值<strong>1</strong>。</p>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; mapdata.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122053.png" alt="image-20200612122052952"></p>
<p>执行<code>reduce</code>操作：</p>
<blockquote>
<p>scala&gt; val reducedata = mapdata.reduceByKey(_+_)</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; reducedata.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122456.png" alt="image-20200612122456444"></p>
<p>返回结果是计算所有字符及它们的出现次数。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/22.%20Spark%E5%AD%97%E7%AC%A6%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B/" data-id="ckblska6x000clktq1ljdcop5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/21. Spark单词统计示例（word count）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/21.%20Spark%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B%EF%BC%88word%20count%EF%BC%89/" class="article-date">
  <time datetime="2020-06-19T05:47:22.562Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/21.%20Spark%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B%EF%BC%88word%20count%EF%BC%89/">Spark学习笔记/21. Spark单词统计示例（word count）</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="21-Spark单词统计示例（word-count）"><a href="#21-Spark单词统计示例（word-count）" class="headerlink" title="21. Spark单词统计示例（word count）"></a>21. Spark单词统计示例（word count）</h1><p>在Spark单词统计示例中，将找出指定文件中存在的每个单词的出现频率。这里我们使用Scala语言来执行Spark操作。</p>
<h4 id="执行Spark字数计算示例的步骤"><a href="#执行Spark字数计算示例的步骤" class="headerlink" title="执行Spark字数计算示例的步骤"></a>执行Spark字数计算示例的步骤</h4><p>在此示例中，查找并显示每个单词的出现次数。在本地服务器中创建一个文本并在其中写入一些单词。</p>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612114032.png" alt="image-20200612114032274"></p>
<p>在hadoop的目录中创建一个目录，保存文本文件：</p>
<blockquote>
<p>$ hdfs dfs -mkdir /spark</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612114904.png" alt="image-20200612114904720"></p>
<p>将sparkdata.txt文件上传到特定目录中：</p>
<blockquote>
<p>$ hdfs dfs -put /root/sparkdata.txt /spark</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115106.png" alt="image-20200612115106288"></p>
<p>在Scala模式下打开Spark:</p>
<blockquote>
<p>$ spark-shell</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115211.png" alt="image-20200612115211199"></p>
<p>创建一个RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.textFile(“../../sparkdata.txt”)</p>
</blockquote>
<p>这里传递包含数据的任何文件名。</p>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115806.png" alt="image-20200612115806248"></p>
<p>以单个单词的形式拆分现有数据（制表符作为分隔符）：</p>
<blockquote>
<p>scala&gt; val splitdata = data.flatMap(line =&gt; line.split(“\t”))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; splitdata.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612120037.png" alt="image-20200612120037176"></p>
<p>执行映射操作：</p>
<blockquote>
<p>scala&gt; val mapdata =  splitdata.map(word =&gt; (word, 1))</p>
</blockquote>
<p>这里每个单词分配值<strong>1</strong>。</p>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; mapdata.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122613.png" alt="image-20200612120241496"></p>
<p>每个单词都有了一个值1.</p>
<p>现在执行<code>reduce</code>操作：</p>
<blockquote>
<p>scala&gt; val reducedata = mapdata.reduceByKey(_+_)</p>
</blockquote>
<p>这里汇总了生成的数据。</p>
<p>读取生成的数据：</p>
<blockquote>
<p>scala&gt; reducedata.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122622.png" alt="image-20200612120428781"></p>
<p>返回结果就是每个单词和他们出现的次数。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/21.%20Spark%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1%E7%A4%BA%E4%BE%8B%EF%BC%88word%20count%EF%BC%89/" data-id="ckblska6z000elktq2tpa45cn" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/20. Spark Take函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/20.%20Spark%20Take%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:47:22.474Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/20.%20Spark%20Take%E5%87%BD%E6%95%B0/">Spark学习笔记/20. Spark Take函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="20-Spark-Take函数"><a href="#20-Spark-Take函数" class="headerlink" title="20. Spark Take函数"></a>20. Spark Take函数</h1><p>在Spark中，<code>take</code>函数的行为类似于数组。它接收一个整数值（比如n）作为参数，并返回数据集的前<code>n</code>个元素的数组。</p>
<h4 id="Take函数示例"><a href="#Take函数示例" class="headerlink" title="Take函数示例"></a>Take函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.parallelize(List(10, 20, 30, 40, 50))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect()</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612113155.png" alt="image-20200612113155754"></p>
<p>应用<code>take()</code>函数来检索数据集的第一个元素（类似first()函数）和前三个元素：</p>
<blockquote>
<p>scala&gt; val takefunc = data.take(1)</p>
<p>scala&gt; val takefunc = data.take(3)</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612113432.png" alt="image-20200612113432394"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/20.%20Spark%20Take%E5%87%BD%E6%95%B0/" data-id="ckblska6x000blktq3u4q7vr1" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/2.Spark架构" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2.Spark%E6%9E%B6%E6%9E%84/" class="article-date">
  <time datetime="2020-06-19T05:47:22.385Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2.Spark%E6%9E%B6%E6%9E%84/">Spark学习笔记/2.Spark架构</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="2-Spark架构"><a href="#2-Spark架构" class="headerlink" title="2.Spark架构"></a>2.Spark架构</h1><p>Spark遵循主从架构。Spark集群由一个主服务器和多个从服务器组成。</p>
<p>Spark架构依赖于两个抽象：</p>
<ol>
<li>​    弹性分布式数据集（RDD）</li>
<li>​    有向无环图（DAG）</li>
</ol>
<h4 id="弹性分布式数据集（RDD）"><a href="#弹性分布式数据集（RDD）" class="headerlink" title="弹性分布式数据集（RDD）"></a>弹性分布式数据集（RDD）</h4><p>弹性分布式数据集就是可以存储在工作节点上的内存中的数据项组。</p>
<ul>
<li>弹性： 失败时恢复数据</li>
<li>分布式： 数据分布在不同的节点之间</li>
<li>数据集： 数据组</li>
</ul>
<h4 id="有向无环图（DAG）"><a href="#有向无环图（DAG）" class="headerlink" title="有向无环图（DAG）"></a>有向无环图（DAG）</h4><p>有向无环图对数据执行一系列计算。每个节点都是RDD分区，边缘是数据顶部的转换。</p>
<h3 id="Spark架构图"><a href="#Spark架构图" class="headerlink" title="Spark架构图"></a>Spark架构图</h3><p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200611154003.png" alt="image-20200611153954186"></p>
<h4 id="驱动程序-Driver-Program"><a href="#驱动程序-Driver-Program" class="headerlink" title="驱动程序 Driver Program"></a>驱动程序 Driver Program</h4><p>驱动程序是一个运行应用程序，由<code>main（）</code>函数创建<code>SparkContext</code>对象的进程。</p>
<p><code>SparkContext</code>的目的是协调spark应用程序，作为集群上的独立进程集运行。</p>
<p>要在集群上运行，<code>SparkContext</code>要连接到不同类型的集群管理器，然后执行以下任务：</p>
<ul>
<li>在集群的节点上获取执行程序</li>
<li>将应用程序代码发送给执行程序。应用程序代码可以通过传递给<code>SparkContext</code>的JAR或者Python文件来定义</li>
<li>最后，<code>SparkContext</code>将任务发送给执行程序以运行</li>
</ul>
<h4 id="集群管理器-Cluster-Manager"><a href="#集群管理器-Cluster-Manager" class="headerlink" title="集群管理器 Cluster Manager"></a>集群管理器 Cluster Manager</h4><p>Spark能够在大量集群上运行需要集群管理器，集群管理器的作用就是跨应用程序分配资源。它由各种类型的集群管理器组成，例如：<code>Hadoop Yarn</code>，<code>Apache Mesos</code>和<code>Standalone scheduler</code>。</p>
<p>这里standalone scheduler是一个独立的Spark集群管理器，便于在一组空机器上安装Spark。</p>
<h4 id="工作节点-Worker-Node"><a href="#工作节点-Worker-Node" class="headerlink" title="工作节点 Worker Node"></a>工作节点 Worker Node</h4><p>工作节点是从节点。它的作用是在集群中运行应用程序代码。也就是实际上干活的。</p>
<h4 id="执行程序-Executor"><a href="#执行程序-Executor" class="headerlink" title="执行程序 Executor"></a>执行程序 Executor</h4><ul>
<li>执行程序是为工作节点上的应用程序启动的进程</li>
<li>执行程序运行任务并将数据保存在内存或者磁盘存储中</li>
<li>它将数据读写到外部源</li>
<li>每个应用程序都包含其执行者(Executor)</li>
</ul>
<h4 id="任务-Task"><a href="#任务-Task" class="headerlink" title="任务 Task"></a>任务 Task</h4><p>任务被发送给一个执行程序的工作单位</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2.Spark%E6%9E%B6%E6%9E%84/" data-id="ckblska6y000dlktqemfegpgo" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/19. Spark First函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/19.%20Spark%20First%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:47:22.383Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/19.%20Spark%20First%E5%87%BD%E6%95%B0/">Spark学习笔记/19. Spark First函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="19-Spark-First函数"><a href="#19-Spark-First函数" class="headerlink" title="19. Spark First函数"></a>19. Spark First函数</h1><p>在Spark中，<code>First</code>函数始终返回数据集的第一个元素。它类似于<code>take(1)</code>。</p>
<h4 id="First函数示例"><a href="#First函数示例" class="headerlink" title="First函数示例"></a>First函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.parallelize(List(10, 20, 30, 40, 50))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612112756.png" alt="image-20200612112756704"></p>
<p>使用<code>first()</code>函数来检索数据集的第一个元素：</p>
<blockquote>
<p>scala&gt; val firstfunc = data.first()</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612112858.png" alt="image-20200612112857994"></p>
<p>返回数据集的第一个元素：10</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/19.%20Spark%20First%E5%87%BD%E6%95%B0/" data-id="ckblska6w000alktqamkdhmgf" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/18. Spark cogroup函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/18.%20Spark%20cogroup%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:47:22.283Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/18.%20Spark%20cogroup%E5%87%BD%E6%95%B0/">Spark学习笔记/18. Spark cogroup函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="18-Spark-cogroup函数"><a href="#18-Spark-cogroup函数" class="headerlink" title="18. Spark cogroup函数"></a>18. Spark cogroup函数</h1><p>在Spark中，<code>cogroup</code>函数对不同的数据集执行分组操作，比如，对(K, V)和(K, W)执行cogroup并返回<code>(K, (Iterable, Iterable))</code>元祖的数据集。此操作也称为<code>groupWith</code>。</p>
<h4 id="cogroup函数示例"><a href="#cogroup函数示例" class="headerlink" title="cogroup函数示例"></a>cogroup函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data1 =  sc.parallelize(Seq((“A”, 1), (“B”, 2), (“C”, 3)))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data1.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612111743.png" alt="image-20200612111743297"></p>
<p>使用并行化集合创建另一个RDD：</p>
<blockquote>
<p>scala&gt; val data2 = sc.parallelize(Seq((“B”, 4), (“E”, 5)))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data2.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612111949.png" alt="image-20200612111949225"></p>
<p>使用<code>cogroup()</code>函数对值进行分组：</p>
<blockquote>
<p>scala&gt; val cogroupfunc = data1.cogroup(data2)</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; cogroupfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612112131.png" alt="image-20200612112131504"></p>
<p>返回的结果是把两个数据集中的所有键包含的值分组在一起。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/18.%20Spark%20cogroup%E5%87%BD%E6%95%B0/" data-id="ckblska6u0008lktqd1au6wl1" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/06/19/Hadoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/7.%20Hadoop%20MapReduce/">Hadoop学习笔记/7. Hadoop MapReduce</a>
          </li>
        
          <li>
            <a href="/2020/06/19/Hadoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%20Hadoop%E5%91%BD%E4%BB%A4%E5%8F%82%E8%80%83/">Hadoop学习笔记/6. Hadoop命令参考</a>
          </li>
        
          <li>
            <a href="/2020/06/19/Hadoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%20Hadoop%20HDFS%E6%93%8D%E4%BD%9C/">Hadoop学习笔记/5. Hadoop HDFS操作</a>
          </li>
        
          <li>
            <a href="/2020/06/19/Hadoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4.%20Hadoop%20HDFS/">Hadoop学习笔记/4. Hadoop HDFS</a>
          </li>
        
          <li>
            <a href="/2020/06/19/Hadoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%20Hadoop%E6%98%AF%E4%BB%80%E4%B9%88/">Hadoop学习笔记/3. Hadoop是什么</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>