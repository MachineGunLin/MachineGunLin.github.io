<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Spark学习笔记/2.Spark架构" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2.Spark%E6%9E%B6%E6%9E%84/" class="article-date">
  <time datetime="2020-06-19T05:47:22.385Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2.Spark%E6%9E%B6%E6%9E%84/">Spark学习笔记/2.Spark架构</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="2-Spark架构"><a href="#2-Spark架构" class="headerlink" title="2.Spark架构"></a>2.Spark架构</h1><p>Spark遵循主从架构。Spark集群由一个主服务器和多个从服务器组成。</p>
<p>Spark架构依赖于两个抽象：</p>
<ol>
<li>​    弹性分布式数据集（RDD）</li>
<li>​    有向无环图（DAG）</li>
</ol>
<h4 id="弹性分布式数据集（RDD）"><a href="#弹性分布式数据集（RDD）" class="headerlink" title="弹性分布式数据集（RDD）"></a>弹性分布式数据集（RDD）</h4><p>弹性分布式数据集就是可以存储在工作节点上的内存中的数据项组。</p>
<ul>
<li>弹性： 失败时恢复数据</li>
<li>分布式： 数据分布在不同的节点之间</li>
<li>数据集： 数据组</li>
</ul>
<h4 id="有向无环图（DAG）"><a href="#有向无环图（DAG）" class="headerlink" title="有向无环图（DAG）"></a>有向无环图（DAG）</h4><p>有向无环图对数据执行一系列计算。每个节点都是RDD分区，边缘是数据顶部的转换。</p>
<h3 id="Spark架构图"><a href="#Spark架构图" class="headerlink" title="Spark架构图"></a>Spark架构图</h3><p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200611154003.png" alt="image-20200611153954186"></p>
<h4 id="驱动程序-Driver-Program"><a href="#驱动程序-Driver-Program" class="headerlink" title="驱动程序 Driver Program"></a>驱动程序 Driver Program</h4><p>驱动程序是一个运行应用程序，由<code>main（）</code>函数创建<code>SparkContext</code>对象的进程。</p>
<p><code>SparkContext</code>的目的是协调spark应用程序，作为集群上的独立进程集运行。</p>
<p>要在集群上运行，<code>SparkContext</code>要连接到不同类型的集群管理器，然后执行以下任务：</p>
<ul>
<li>在集群的节点上获取执行程序</li>
<li>将应用程序代码发送给执行程序。应用程序代码可以通过传递给<code>SparkContext</code>的JAR或者Python文件来定义</li>
<li>最后，<code>SparkContext</code>将任务发送给执行程序以运行</li>
</ul>
<h4 id="集群管理器-Cluster-Manager"><a href="#集群管理器-Cluster-Manager" class="headerlink" title="集群管理器 Cluster Manager"></a>集群管理器 Cluster Manager</h4><p>Spark能够在大量集群上运行需要集群管理器，集群管理器的作用就是跨应用程序分配资源。它由各种类型的集群管理器组成，例如：<code>Hadoop Yarn</code>，<code>Apache Mesos</code>和<code>Standalone scheduler</code>。</p>
<p>这里standalone scheduler是一个独立的Spark集群管理器，便于在一组空机器上安装Spark。</p>
<h4 id="工作节点-Worker-Node"><a href="#工作节点-Worker-Node" class="headerlink" title="工作节点 Worker Node"></a>工作节点 Worker Node</h4><p>工作节点是从节点。它的作用是在集群中运行应用程序代码。也就是实际上干活的。</p>
<h4 id="执行程序-Executor"><a href="#执行程序-Executor" class="headerlink" title="执行程序 Executor"></a>执行程序 Executor</h4><ul>
<li>执行程序是为工作节点上的应用程序启动的进程</li>
<li>执行程序运行任务并将数据保存在内存或者磁盘存储中</li>
<li>它将数据读写到外部源</li>
<li>每个应用程序都包含其执行者(Executor)</li>
</ul>
<h4 id="任务-Task"><a href="#任务-Task" class="headerlink" title="任务 Task"></a>任务 Task</h4><p>任务被发送给一个执行程序的工作单位</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2.Spark%E6%9E%B6%E6%9E%84/" data-id="ckblska6y000dlktqemfegpgo" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/19. Spark First函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/19.%20Spark%20First%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:47:22.383Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/19.%20Spark%20First%E5%87%BD%E6%95%B0/">Spark学习笔记/19. Spark First函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="19-Spark-First函数"><a href="#19-Spark-First函数" class="headerlink" title="19. Spark First函数"></a>19. Spark First函数</h1><p>在Spark中，<code>First</code>函数始终返回数据集的第一个元素。它类似于<code>take(1)</code>。</p>
<h4 id="First函数示例"><a href="#First函数示例" class="headerlink" title="First函数示例"></a>First函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.parallelize(List(10, 20, 30, 40, 50))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612112756.png" alt="image-20200612112756704"></p>
<p>使用<code>first()</code>函数来检索数据集的第一个元素：</p>
<blockquote>
<p>scala&gt; val firstfunc = data.first()</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612112858.png" alt="image-20200612112857994"></p>
<p>返回数据集的第一个元素：10</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/19.%20Spark%20First%E5%87%BD%E6%95%B0/" data-id="ckblska6w000alktqamkdhmgf" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/18. Spark cogroup函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/18.%20Spark%20cogroup%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:47:22.283Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/18.%20Spark%20cogroup%E5%87%BD%E6%95%B0/">Spark学习笔记/18. Spark cogroup函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="18-Spark-cogroup函数"><a href="#18-Spark-cogroup函数" class="headerlink" title="18. Spark cogroup函数"></a>18. Spark cogroup函数</h1><p>在Spark中，<code>cogroup</code>函数对不同的数据集执行分组操作，比如，对(K, V)和(K, W)执行cogroup并返回<code>(K, (Iterable, Iterable))</code>元祖的数据集。此操作也称为<code>groupWith</code>。</p>
<h4 id="cogroup函数示例"><a href="#cogroup函数示例" class="headerlink" title="cogroup函数示例"></a>cogroup函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data1 =  sc.parallelize(Seq((“A”, 1), (“B”, 2), (“C”, 3)))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data1.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612111743.png" alt="image-20200612111743297"></p>
<p>使用并行化集合创建另一个RDD：</p>
<blockquote>
<p>scala&gt; val data2 = sc.parallelize(Seq((“B”, 4), (“E”, 5)))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data2.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612111949.png" alt="image-20200612111949225"></p>
<p>使用<code>cogroup()</code>函数对值进行分组：</p>
<blockquote>
<p>scala&gt; val cogroupfunc = data1.cogroup(data2)</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; cogroupfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612112131.png" alt="image-20200612112131504"></p>
<p>返回的结果是把两个数据集中的所有键包含的值分组在一起。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/18.%20Spark%20cogroup%E5%87%BD%E6%95%B0/" data-id="ckblska6u0008lktqd1au6wl1" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/17. Spark reduceByKey函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/17.%20Spark%20reduceByKey%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:47:22.193Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/17.%20Spark%20reduceByKey%E5%87%BD%E6%95%B0/">Spark学习笔记/17. Spark reduceByKey函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="17-Spark-reduceByKey函数"><a href="#17-Spark-reduceByKey函数" class="headerlink" title="17. Spark reduceByKey函数"></a>17. Spark reduceByKey函数</h1><p>在spark中，<code>reduceByKey</code>函数是一种常用的转换操作，它执行数据聚合（聚合的规则自己定义）。</p>
<p>它接收键值对(K, V)作为输入，基于键聚合值并生成(K, V）对的数据集作为输出。</p>
<h4 id="reduceByKey函数示例"><a href="#reduceByKey函数示例" class="headerlink" title="reduceByKey函数示例"></a>reduceByKey函数示例</h4><p>在此示例中，我们基于键聚合值.</p>
<p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.parallelize(Array((“C”, 3), (“A”, 1), (“B”, 4), (“A”, 2), (“B”, 5)))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612111753.png" alt="image-20200612105256248"></p>
<p>应用<code>reduceByKey()</code>函数来聚合值。</p>
<blockquote>
<p>scala&gt; val reducefunc = data.reduceByKey((value, x) =&gt; (value + x))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; reducefunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612111805.png" alt="image-20200612105529453"></p>
<p>这里聚合的规则是把键相同的值都加起来，返回结果是键值对，每个键的值是原来数据集中这个键的所有值的总和。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/17.%20Spark%20reduceByKey%E5%87%BD%E6%95%B0/" data-id="ckblska6v0009lktqhufba2f2" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/16. Spark groupByKey函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/16.%20Spark%20groupByKey%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:47:22.107Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/16.%20Spark%20groupByKey%E5%87%BD%E6%95%B0/">Spark学习笔记/16. Spark groupByKey函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="16-Spark-groupByKey函数"><a href="#16-Spark-groupByKey函数" class="headerlink" title="16. Spark groupByKey函数"></a>16. Spark groupByKey函数</h1><p>在spark中，<code>groupByKey</code>函数是一种经常使用的转换操作，它执行数据的<strong>混乱</strong>。</p>
<p>它接收键值对(K, V)作为输入，基于键值对进行分组，<strong><em>key相同的value被分到一组</em></strong>。并生成（K, Iterable）对的数据集作为输出。</p>
<h4 id="groupByKey函数示例"><a href="#groupByKey函数示例" class="headerlink" title="groupByKey函数示例"></a>groupByKey函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.parallelize(Seq((“C”, 3), (“A”, 1), (“ B”, 4), (“A”, 2), (“B”, 5)))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612104346.png" alt="image-20200612104346925"></p>
<p>应用<code>groupByKey()</code>函数对键值对进行分组：</p>
<blockquote>
<p>scala&gt; val groupfunc = data.groupByKey()</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; groupfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612104534.png" alt="image-20200612104534462"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/16.%20Spark%20groupByKey%E5%87%BD%E6%95%B0/" data-id="ckblska6t0006lktqez4v7gg9" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/15. Spark sortByKey函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/15.%20Spark%20sortByKey%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:47:22.037Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/15.%20Spark%20sortByKey%E5%87%BD%E6%95%B0/">Spark学习笔记/15. Spark sortByKey函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="15-Spark-sortByKey函数"><a href="#15-Spark-sortByKey函数" class="headerlink" title="15. Spark sortByKey函数"></a>15. Spark sortByKey函数</h1><p>在Spark中，<code>sortByKey</code>函数维护元素的顺序。它接收键值对<code>(K, V)</code>作为输入，按升序或降序对元素进行排序，并按顺序生成数据集。</p>
<h4 id="sortByKey函数示例"><a href="#sortByKey函数示例" class="headerlink" title="sortByKey函数示例"></a>sortByKey函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.parallelize(Seq((“C”, 3), (“A”, 1), (“D”, 4), (“B”, 2), (“E”, 5)))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612103346.png" alt="image-20200612103346422"></p>
<p>应用<code>sortByKey()</code>函数来对数据集进行排序：</p>
<blockquote>
<p>scala&gt; val sortfunc = data.sortByKey()</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; sortfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612103512.png" alt="image-20200612103512106"></p>
<p>返回结果按照K-V对的key进行排序，这里的key是字母，返回结果按照字典序升序排序。</p>
<p>对于降序，应用<code>sortByKey()</code>函数并将布尔类型作为参数传递。</p>
<blockquote>
<p>scala&gt; val sortfunc = data.sortByKey(false)</p>
</blockquote>
<p>读取生成的结果:</p>
<blockquote>
<p>scala&gt; sortfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612103722.png" alt="image-20200612103722121"></p>
<p>sortByKey函数传递一个<code>false</code>参数，最后的返回结果就是按照字典序降序。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/15.%20Spark%20sortByKey%E5%87%BD%E6%95%B0/" data-id="ckblska6u0007lktq2eyd8qmu" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/14. Spark Cartesian函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/14.%20Spark%20Cartesian%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:47:21.939Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/14.%20Spark%20Cartesian%E5%87%BD%E6%95%B0/">Spark学习笔记/14. Spark Cartesian函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="14-Spark-Cartesian函数"><a href="#14-Spark-Cartesian函数" class="headerlink" title="14. Spark Cartesian函数"></a>14. Spark Cartesian函数</h1><p>在Spark中，<code>cartesian</code>函数生成两个数据集的笛卡尔积，并返回所有可能的组合对。</p>
<p>这里，一个数据集的每一个元素都与另一个数据集的每个元素配对。</p>
<h4 id="Cartesian函数示例"><a href="#Cartesian函数示例" class="headerlink" title="Cartesian函数示例"></a>Cartesian函数示例</h4><p>在此示例中，生成两个数据的笛卡尔积。</p>
<p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data1 = sc.parallelize(List(1, ,2, 3))</p>
</blockquote>
<p>读取生成的结果:</p>
<blockquote>
<p>scala&gt; data1.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612101947.png" alt="image-20200612101947084"></p>
<p>使用并行化集合创建另一个RDD：</p>
<blockquote>
<p>scala&gt; val data2 = sc.parallelize(List(3, 4, 5))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data2.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612102109.png" alt="image-20200612102109091"></p>
<p>应用<code>cartesian()</code>函数返回两个数据集的笛卡尔积:</p>
<blockquote>
<p>scala&gt; val cartesianfunc = data1.cartesian(data2)</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; cartesianfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612102315.png" alt="image-20200612102315863"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/14.%20Spark%20Cartesian%E5%87%BD%E6%95%B0/" data-id="ckblska6r0003lktqaoe2gmww" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/13. Spark Intersection函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.%20Spark%20Intersection%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:47:21.810Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.%20Spark%20Intersection%E5%87%BD%E6%95%B0/">Spark学习笔记/13. Spark Intersection函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="13-Spark-Intersection函数"><a href="#13-Spark-Intersection函数" class="headerlink" title="13. Spark Intersection函数"></a>13. Spark Intersection函数</h1><p>在Spark中，<code>Intersection</code>函数返回一个新数据集，其中包含不同数据集中存在的元素的<strong><em>交集</em></strong>。因此它只返回一行。此函数的行为与SQL中的<code>INTERSECT</code>查询类似。</p>
<h4 id="Intersection函数示例"><a href="#Intersection函数示例" class="headerlink" title="Intersection函数示例"></a>Intersection函数示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data1 = sc.parallelzie(List(1, 2 , 3))</p>
</blockquote>
<p>读取生成的结果:</p>
<blockquote>
<p>scala&gt; data1.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612100650.png" alt="image-20200612100650624"></p>
<p>使用并行化集合创建另一个RDD：</p>
<blockquote>
<p>scala&gt; val data2 = sc.parallelize(List(3, 4, 5))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data2.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612100804.png" alt="image-20200612100804900"></p>
<p>应用<code>intersection()</code>函数返回两个数据集的交集:</p>
<blockquote>
<p>scala&gt; val intersectfunc = data1.intersection(data2)</p>
</blockquote>
<p>读取生成的结果:</p>
<blockquote>
<p>scala&gt; intersectfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612100934.png" alt="image-20200612100933990"></p>
<p>返回结果是两个集合的交集: 3</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.%20Spark%20Intersection%E5%87%BD%E6%95%B0/" data-id="ckblska6s0005lktqeta06nj0" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/12. Spark Union函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/12.%20Spark%20Union%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:47:21.505Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/12.%20Spark%20Union%E5%87%BD%E6%95%B0/">Spark学习笔记/12. Spark Union函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="12-Spark-Union函数"><a href="#12-Spark-Union函数" class="headerlink" title="12. Spark Union函数"></a>12. Spark Union函数</h1><p>在Spark中，<code>Union</code>函数返回一个新数据集，其中包含不同数据集中存在的元素组合（也就是说返回两个数据集的并集）。</p>
<h4 id="Union函数示例"><a href="#Union函数示例" class="headerlink" title="Union函数示例"></a>Union函数示例</h4><p>在此示例中，组合了两个元素的数据集。</p>
<p>使用并行化集合创建集合RDD：</p>
<blockquote>
<p>scala&gt; val data1 = sc.parallelize(List(1, 2))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data1.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612095401.png" alt="image-20200612095229224"></p>
<p>使用并行化集合创建另一个RDD：</p>
<blockquote>
<p>scala&gt; val data2 = sc.parallelize(List(3, 4, 5))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data2.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612095351.png" alt="image-20200612095351250"></p>
<p>应用<code>union()</code>函数返回元素的并集:</p>
<blockquote>
<p>scala&gt; val unionfunc = data1.union(data2)</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; unionfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612095525.png" alt="image-20200612095525301"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/12.%20Spark%20Union%E5%87%BD%E6%95%B0/" data-id="ckblska6s0004lktqhxie8xeg" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Spark学习笔记/11. Spark Distinct函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/11.%20Spark%20Distinct%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-06-19T05:47:21.432Z" itemprop="datePublished">2020-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/11.%20Spark%20Distinct%E5%87%BD%E6%95%B0/">Spark学习笔记/11. Spark Distinct函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="11-Spark-Distinct函数"><a href="#11-Spark-Distinct函数" class="headerlink" title="11. Spark Distinct函数"></a>11. Spark Distinct函数</h1><p>在Spark中，<code>Distinct</code>函数返回提供的数据集中的不同元素（相当于数据集全都放入一个set，再返回set中的所有元素）。</p>
<h4 id="Distinct函数的示例"><a href="#Distinct函数的示例" class="headerlink" title="Distinct函数的示例"></a>Distinct函数的示例</h4><p>使用并行化集合创建RDD：</p>
<blockquote>
<p>scala&gt; val data = sc.parallelize(List(10, 20, 20, 40))</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; data.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612095546.png" alt="image-20200612094720307"></p>
<p>使用<code>distinct()</code>函数来忽略重复的元素：</p>
<blockquote>
<p>scala&gt; val distinctfunc = data.distinct()</p>
</blockquote>
<p>读取生成的结果：</p>
<blockquote>
<p>scala&gt; distinctfunc.collect</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612095554.png" alt="image-20200612094841086"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/11.%20Spark%20Distinct%E5%87%BD%E6%95%B0/" data-id="ckblska6q0002lktqb3y3g013" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/9.%20Spark%20Filter%E5%87%BD%E6%95%B0/">Spark学习笔记/9. Spark Filter函数</a>
          </li>
        
          <li>
            <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/8.%20Spark%20Map%E5%87%BD%E6%95%B0/">Spark学习笔记/8. Spark Map函数</a>
          </li>
        
          <li>
            <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/7.%20RDD%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F/">Spark学习笔记/7. RDD共享变量</a>
          </li>
        
          <li>
            <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%20RDD%E6%8C%81%E4%B9%85%E5%8C%96/">Spark学习笔记/6. RDD持久化</a>
          </li>
        
          <li>
            <a href="/2020/06/19/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.RDD%E6%93%8D%E4%BD%9C/">Spark学习笔记/5.RDD操作</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>